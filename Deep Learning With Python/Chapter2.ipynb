{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2\n",
    "### This chapter covers\n",
    "- A first example of a neural network\n",
    "- Tensors and tensor operations\n",
    "- How neural networks learn via backpropagation\n",
    "and gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We simple learn mathmatical concept and, know about tensor, tensor operation and neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 A first look at a neural network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Classification Problem Hand writing number (0 to 9) 28X28 pixels\n",
    "- Use MNIST Data set\n",
    "- See some MNIST samples in figure 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note on classes and labels\n",
    "In machine learning, a category in a classification problem is called a class. Data\n",
    "points are called samples. The class associated with a specific sample is called a\n",
    "label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"images/f2.1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Keras to solve above problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muhammad Qasim\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# 2.1\n",
    "from keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mnist.load_data()` this function reuturn four values \n",
    "- train_data, train_labels (60% from all data)\n",
    "- test_data, test_labels (40% from all data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our images convert in numpuy array with value of grayscale mode (R,G,B convert into GrayScale= R+G+B/3)\n",
    "#### Labels are in 0 to 9\n",
    "\n",
    "## About Train Data Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train Image Shape: \", train_images.shape)\n",
    "print(\"Len of train labels:\", len(train_labels))\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Test Data Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train Image Shape: \", test_images.shape)\n",
    "print(\"Len of train labels:\", len(test_labels))\n",
    "test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets Try to train our module \n",
    "- learn train label from train data\n",
    "- predict label on test data then match with test data labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 2.2 The network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- core think in network is layer\n",
    "- every layer work like filter where data through in layer and it comes out in more usefull form\n",
    "-  layers extract representations out of the data fed into them\n",
    "- deep learning consists of chaining together simple layers that will implement a form\n",
    "of progressive data distillation.\n",
    "- Deep learning model like a sieve for data processing, made of a succession of increasingly refined data filters \n",
    "- <b>our network consists of a sequence of two Dense layers,</b>  which are densely\n",
    "connected (also called fully connected) neural layers. \n",
    "The second (and last) layer is a 10-way softmax layer, that return of 10 probility scores (suming to 1) each score probility the current digit image belong on of our 10 digit classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To make the network ready for training, we need to pick three more things, as part\n",
    "of the compilation step:\n",
    "- loss function\n",
    "- An optimizer\n",
    "- Metrics to monitor during training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 2.3 The compilation step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Before training, we’ll preprocess the data by reshaping it into the shape the network\n",
    "expects and scaling it so that all values are in the [0, 1] interval. Previously, our training\n",
    "images, for instance, were stored in an array of shape (60000, 28, 28) of type\n",
    "uint8 with values in the [0, 255] interval. We transform it into a float32 array of\n",
    "shape (60000, 28 * 28) with values between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 2.4 Preparing the image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 2.5 Preparing the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’re now ready to train the network, which in Keras is done via a call to the network’s\n",
    "fit method—we fit the model to its training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " network.fit(train_images, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We quickly reach an accuracy of 0.989 (98.9%) on the training data. Now let’s\n",
    "check that the model performs well on the test set, too:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " test_loss, test_acc = network.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what’s going on behind the scenes. You’ll learn about tensors, the data-storing objects\n",
    "going into the network; tensor operations, which layers are made of; and gradient\n",
    "descent, which allows your network to learn from its training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Data representations for neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  At its core, a tensor is a container for data—almost always numerical data.\n",
    "- which are 2D tensors:\n",
    "tensors are a generalization of matrices to an arbitrary number of dimensions\n",
    "(note that in the context of tensors, a dimension is often called an axis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.1 Scalars (0D tensors)\n",
    "- A tensor that contain just one number is call Scalar\n",
    "- a scalar\n",
    "tensor has 0 axes (ndim == 0).The number of axes of a tensor is also called its rank.\n",
    "Here’s a Numpy scalar:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.array(12)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.2 Vectors (1D tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Array of numbers is called vector, id tensor, a ID tensor is said to have exactly one axis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([12,3,6,14])\n",
    "print(x)\n",
    "print(x.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This vector has five entries and so is called a 5-dimensional vector\n",
    "- whereas a 5D tensor has five axes  (and may have any number of dimensions\n",
    "along each axis).\n",
    "-  it’s technically more\n",
    "correct to talk about a tensor of rank 5 (the rank of a tensor being the number of axes),\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2.3 Matrices (2D tensors)\n",
    "- An array of vectors is a matrix, or 2D tensor. A matrix has two axes (often referred to\n",
    "rows and columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[5, 78, 2, 34, 0],\n",
    "              [6, 79, 3, 35, 1],\n",
    "              [7, 80, 4, 36, 2]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2.4 3D tensors and higher-dimensional tensors\n",
    "-  which you can visually\n",
    "interpret as a cube of numbers. Following is a Numpy 3D tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[[5, 78, 2, 34, 0],\n",
    "               [6, 79, 3, 35, 1],\n",
    "               [7, 80, 4, 36, 2]],\n",
    "              [[5, 78, 2, 34, 0],\n",
    "               [6, 79, 3, 35, 1],\n",
    "               [7, 80, 4, 36, 2]],\n",
    "              [[5, 78, 2, 34, 0],\n",
    "               [6, 79, 3, 35, 1],\n",
    "               [7, 80, 4, 36, 2]]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2.5 Key attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A tensor is defined by three key attributes:\n",
    "- <b>Number of axes (rank)</b>:<br>For instance, a 3D tensor has three axes, and a matrix has\n",
    "two axes. This is also called the tensor’s ndim in Python libraries such as Numpy.\n",
    "- <b>Shape</b>:<br>shape\n",
    "(3, 5), and the 3D tensor example has shape (3, 3, 5). A vector has a shape\n",
    "with a single element, such as (5,), whereas a scalar has an empty shape, ().\n",
    "- <b>Data type</b>:\n",
    "     tensor’s type could be float32, uint8,\n",
    "float64, and so on. On rare occasions, you may see a char tensor. Note that\n",
    "string tensors don’t exist in Numpy (or in most other libraries),To make this more concrete, let’s look back at the data we processed in the MNIST\n",
    "example. First, we load the MNIST dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we display the number of axes of the tensor train_images, the ndim attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_images.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_images.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what we have here is a 3D tensor of 8-bit integers. More precisely, it’s an array of\n",
    "60,000 matrices of 28 × 8 integers. Each such matrix is a grayscale image, with coefficients\n",
    "between 0 and 255.\n",
    " Let’s display the fourth digit in this 3D tensor, using the library Matplotlib (part of\n",
    "the standard scientific Python suite); see figure 2.2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 2.6 Displaying the fourth digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit = train_images[4]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(digit, cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.6 Manipulating tensors in Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_slice = train_images[10:100]\n",
    "my_slice.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_slice = train_images[10:100, :, :]\n",
    "my_slice.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_slice = train_images[10:100, 0:28, 0:28]\n",
    "my_slice.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s also possible to use negative indices. Much like negative indices in Python lists,\n",
    "they indicate a position relative to the end of the current axis. In order to crop the\n",
    "images to patches of 14 × 14 pixels centered in the middle, you do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_slice = train_images[:, 7:-7, 7:-7]\n",
    "my_slice.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.7 The notion of data batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the first axis (axis 0, because indexing starts at 0) in all data tensors you’ll\n",
    "come across in deep learning will be the samples axis (sometimes called the samples\n",
    "dimension). \n",
    "-  In addition, deep-learning models don’t process an entire dataset at once; rather,\n",
    "they break the data into small batches. Concretely, here’s one batch of our MNIST digits,\n",
    "with batch size of 128:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = train_images[:128]\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here’s the next batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch = train_images[128:256]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the nth batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch = train_images[128 * n:128 * (n + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>, the first axis (axis 0) is called the batch axis or\n",
    "batch dimension. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.8 Real-world examples of data tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s make data tensors more concrete with a few examples similar to what you’ll\n",
    "encounter later. The data you’ll manipulate will almost always fall into one of the following\n",
    "categories:\n",
    "- <b>Vector data—2D</b> tensors of shape (samples, features)\n",
    "- <b>Timeseries data</b> or sequence data—3D tensors of shape (samples, timesteps,\n",
    "features)\n",
    "- <b>Images—4D tensors</b> of shape (samples, height, width, channels) or (samples,\n",
    "channels, height, width)\n",
    "- <b>Video—5D tensors</b> of shape (samples, frames, height, width, channels) or\n",
    "(samples, frames, channels, height, width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.9 Vector data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(that is, an array of\n",
    "vectors), where the first axis is the samples axis and the second axis is the features axis.\n",
    " Let’s take a look at two examples:\n",
    "-  person’s age, ZIP code,\n",
    "and income.\n",
    "    - 100,000 people can be stored in a 2D tensor of shape\n",
    "(100000, 3)\n",
    "- A dataset of text documents, where we represent each document by the counts\n",
    "of how many times each word appears in it (out of a dictionary of 20,000 common\n",
    "words). Each document can be encoded as a vector of 20,000 values (one\n",
    "count per word in the dictionary), and thus an entire dataset of 500 documents\n",
    "can be stored in a tensor of shape (500, 20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.10 Timeseries data or sequence data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sample can be encoded as a\n",
    "sequence of vectors (a 2D tensor), and thus a batch of data will be encoded as a 3D\n",
    "tensor (see figure 2.3).<img src='images/f2.3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The time axis is always the second axis (axis of index 1), by convention. Let’s look at a\n",
    "few examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Thus every minute is encoded as a 3D vector, an entire day of trading is\n",
    "encoded as a 2D tensor of shape (390, 3) (there are 390 minutes in a trading\n",
    "day), and 250 days’ worth of data can be stored in a 3D tensor of shape (250,\n",
    "390, 3). Here, each sample would be one day’s worth of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Then each tweet can be encoded\n",
    "as a 2D tensor of shape (280, 128), and a dataset of 1 million tweets can be\n",
    "stored in a tensor of shape (1000000, 280, 128).`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.11 Image data\n",
    "<img src='images/f2.4.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dummy Data\n",
    "image = np.random.randn(128, 256, 256, 3)\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- height, width, and color depth.\n",
    "- samples, width,height,chanel (TensorFlow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randn(128, 256, 256, 3)\n",
    "data[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.12 Video data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (samples, frames, height, width, color_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 The gears of neural networks: tensor operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In our initial example, we were building our network by stacking Dense layers on\n",
    "top of each other. A Keras layer instance looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.layers.Dense(512, activation='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def naive_relu(x):\n",
    "    assert len(x.shape) == 2\n",
    "    x = x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] = max(x[i, j], 0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def naive_add(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert x.shape == y.shape\n",
    "    \n",
    "    x = x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] += y[i, j]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in Numpy, you can do the following element-wise operation, and it will be blazing\n",
    "fast:\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "z=x+y\n",
    "z = np.maximum(z, 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3.2 Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the smaller tensor will be broadcasted to\n",
    "match the shape of the larger tensor.\n",
    "- Axes (called broadcast axes) are added to the smaller tensor to match the ndim of\n",
    "the larger tensor.\n",
    "- Axes (called broadcast axes) are added to the smaller tensor to match the ndim of\n",
    "the larger tensor.\n",
    "    - `X shape (32,10) <br> y shape (10,)<br> (1,10)<br>(32, 10), where Y[i, :] == y for i in range(0, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def naive_add_matrix_and_vector(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    x = x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] += y[j]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.random.random((64, 3, 32, 10))\n",
    "y = np.random.random((32, 10))\n",
    "z = np.maximum(x, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output z has shape\n",
    "(64, 3, 32, 10) like x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.3 Tensor dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot operation, also called a tensor product (not to be confused with an elementwise\n",
    "product) is the most common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.random.randn(10)\n",
    "y = np.random.randn(10)\n",
    "z = np.dot(x, y)\n",
    "print(x)\n",
    "print(yIn mathematical notation, you’d note the operation with a dot (.):)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In mathematical notation, you’d note the operation with a dot (.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z = x . y\n",
    "\n",
    "def naive_vector_dot(x, y):\n",
    "    assert len(x.shape) == 1\n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[0] == y.shape[0]\n",
    "    \n",
    "    z = 0.\n",
    "    for i in range(x.shape[0]):\n",
    "        z += x[i] * y[i]\n",
    "    return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def naive_matrix_vector_dot(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    z = np.zeros(x.shape[0])\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            z[i] += x[i, j] * y[j]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could also reuse the code we wrote previously, which highlights the relationship\n",
    "between a matrix-vector product and a vector product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def naive_matrix_vector_dot(x, y):\n",
    "    z = np.zeros(x.shape[0])\n",
    "    for i in range(x.shape[0]):\n",
    "        z[i] = naive_vector_dot(x[i, :], y)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_matrix_dot(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 2\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    z = np.zeros((x.shape[0], y.shape[1]))\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(y.shape[1]):\n",
    "            row_x = x[i, :]\n",
    "            column_y = y[:, j]\n",
    "            z[i, j] = naive_vector_dot(row_x, column_y)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/f2.5.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More generally, you can take the dot product between higher-dimensional tensors,\n",
    "following the same rules for shape compatibility as outlined earlier for the 2D case:\n",
    "```\n",
    "(a, b, c, d) . (d,) -> (a, b, c)\n",
    "(a, b, c, d) . (d, e) -> (a, b, c, e)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.4 Tensor reshaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " preprocessed the digits data before feeding it into our network\n",
    "```train_images = train_images.reshape((60000, 28 * 28)) ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array([[0., 1.],\n",
    "[2., 3.],\n",
    "[4., 5.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.reshape(1,6)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.reshape(6,1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((300, 20))\n",
    "x = np.transpose(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometric interpretation of tensor operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, let’s consider addition. We’ll start with the following\n",
    "vector:\n",
    "` A = [0.5, 1]`\n",
    "<img src='images/f2.6.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s consider a new point, B = [1, 0.25],\n",
    " the resulting location\n",
    "being the vector representing the sum of the previous two vectors (see figure 2.8).<img src='images/f2.8.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, elementary geometric operations such as affine transformations, rotations,\n",
    "scaling, and so on can be expressed as tensor operations. For instance, a rotation of a\n",
    "2D vector by an angle theta can be achieved via a dot product with a 2 × 2 matrix\n",
    "R = [u, v], where u and v are both vectors of the plane: u = [cos(theta),\n",
    "sin(theta)] and v = [-sin(theta), cos(theta)]. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.6 A geometric interpretation of deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/f2.9.png'>\n",
    " Each layer in a deep network applies a transformation that\n",
    "disentangles the data a little—and a deep stack of layers makes tractable an extremely\n",
    "complicated disentanglement process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 The engine of neural networks:<br>gradient-based optimization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This happens within what’s called a training loop, which works as follows. Repeat\n",
    "these steps in a loop, as long as necessary:\n",
    "1 Draw a batch of training samples x and corresponding targets y.\n",
    "2 Run the network on x (a step called the forward pass) to obtain predictions y_pred.\n",
    "3 Compute the loss of the network on the batch, a measure of the mismatch\n",
    "between y_pred and y.\n",
    "4 Update all weights of the network in a way that slightly reduces the loss on this\n",
    "batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.1 What’s a derivative?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "f(x + epsilon_x) = y + epsilon_y\n",
    "f(x + epsilon_x) = y + a * epsilon_x\n",
    "```\n",
    "Obviously, this linear approximation is valid only when x is close enough to p.\n",
    " The slope a is called the derivative of f in p. If a is negative, it means a small change\n",
    "of x around p will result in a decrease of f(x) (as shown in figure 2.10); and if a is positive,\n",
    "a small change in x will result in an increase of f(x). Further, the absolute value\n",
    "of a (the magnitude of the derivative) tells you how quickly this increase or decrease\n",
    "will happen.\n",
    "<img src='images/f2.10.png'>\n",
    " If you’re trying to update x by a factor epsilon_x in order to minimize f(x), and\n",
    "you know the derivative of f, then your job is done: the derivative completely\n",
    "describes how f(x) evolves as you change x. If you want to reduce the value of f(x),\n",
    "you just need to move x a little in the opposite direction from the derivative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.2 Derivative of a tensor operation: the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Consider an input vector x, a matrix W, a target y, and a loss function loss. You can\n",
    "use W to compute a target candidate y_pred, and compute the loss, or mismatch,\n",
    "between the target candidate y_pred and the target y:\n",
    "<br>`y_pred = dot(W, x)`<br>\n",
    "`loss_value = loss(y_pred, y)`<br>\n",
    "If the data inputs x and y are frozen, then this can be interpreted as a function mapping\n",
    "values of W to loss values:<br>\n",
    "`loss_value = f(W)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.3 Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Draw a batch of training samples x and corresponding targets y.\n",
    "2. Run the network on x to obtain predictions y_pred.\n",
    "3. Compute the loss of the network on the batch, a measure of the mismatch\n",
    "between y_pred and y.\n",
    "4. Compute the gradient of the loss with regard to the network’s parameters (a\n",
    "backward pass).\n",
    "5. Move the parameters a little in the opposite direction from the gradient—for\n",
    "example W -= step * gradient—thus reducing the loss on the batch a bit.\n",
    "<img src='images/f2.11.png'>\n",
    "<img src='images/f2.12.png'>\n",
    "<img src='images/f2.13.png'>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "past_velocity = 0.\n",
    "momentum = 0.1\n",
    "while loss > 0.01:\n",
    "    w, loss, gradient = get_current_parameters()\n",
    "    velocity = past_velocity * momentum + learning_rate * gradient\n",
    "    w=w+ momentum * velocity - learning_rate * gradient\n",
    "    past_velocity = velocity\n",
    "    update_parameter(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.4 Chaining derivatives: the Backpropagation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "three tensor operations,\n",
    "a, b, and c, with weight matrices W1, W2, and W3:\n",
    "`f(W1, W2, W3) = a(W1, b(W2, c(W3)))`\n",
    "Calculus tells us that such a chain of functions can be derived using the following identity,\n",
    "called the chain rule: `f(g(x)) = f'(g(x)) * g'(x)`. Applying the chain rule to the\n",
    "computation of the gradient values of a neural network gives rise to an algorithm\n",
    "called Backpropagation (also sometimes called reverse-mode differentiation). Backpropagation\n",
    "starts with the final loss value and works backward from the top layers to the bottom\n",
    "layers, applying the chain rule to compute the contribution that each parameter\n",
    "had in the loss value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Looking back at our first example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ve reached the end of this chapter, and you should now have a general understanding\n",
    "of what’s going on behind the scenes in a neural network. Let’s go back to\n",
    "the first example and review each piece of it in the light of what you’ve learned in the\n",
    "previous three sections.\n",
    " This was the input data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you understand that the input images are stored in Numpy tensors, which are\n",
    "here formatted as float32 tensors of shape (60000, 784) (training data) and (10000,\n",
    "784) (test data), respectively.\n",
    " This was our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you understand that this network consists of a chain of two Dense layers, that\n",
    "each layer applies a few simple tensor operations to the input data, and that these\n",
    "operations involve weight tensors. Weight tensors, which are attributes of the layers,\n",
    "are where the knowledge of the network persists.\n",
    " This was the network-compilation step:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
    "#train_images.shape\n",
    "#train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter summary\n",
    "1. Learning means finding a combination of model parameters that minimizes\n",
    "a loss function for a given set of training data samples and their corresponding\n",
    "targets.\n",
    "2. Learning happens by drawing random batches of data samples and their\n",
    "targets, and computing the gradient of the network parameters with\n",
    "respect to the loss on the batch. The network parameters are then moved\n",
    "a bit (the magnitude of the move is defined by the learning rate) in the\n",
    "opposite direction from the gradient.\n",
    "3. The entire learning process is made possible by the fact that neural networks\n",
    "are chains of differentiable tensor operations, and thus it’s possible\n",
    "to apply the chain rule of derivation to find the gradient function mapping\n",
    "the current parameters and current batch of data to a gradient value.\n",
    "4. Two key concepts you’ll see frequently in future chapters are loss and optimizers.\n",
    "These are the two things you need to define before you begin feeding\n",
    "data into a network.\n",
    "5. The loss is the quantity you’ll attempt to minimize during training, so it\n",
    "should represent a measure of success for the task you’re trying to solve.\n",
    "6. The optimizer specifies the exact way in which the gradient of the loss will\n",
    "be used to update parameters: for instance, it could be the RMSProp optimizer,\n",
    "SGD with momentum, and so on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
