{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of machine learning\n",
    "________\n",
    "<!--\n",
    "Author Muhammad Qasim\n",
    "Date: 19-March-2018\n",
    "-->\n",
    "<style>\n",
    ".gray{\n",
    "background-color: #ccc !important;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This chapter covers\n",
    "\n",
    "- Forms of machine learning beyond classification\n",
    "and regression\n",
    "- Formal evaluation procedures for machinelearning\n",
    "models\n",
    "- Preparing data for deep learning\n",
    "- Feature engineering\n",
    "- Tackling overfitting\n",
    "- The universal workflow for approaching machinelearning\n",
    "problems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Four branches of machine learning\n",
    " binary classification, multiclass classification, and scalar\n",
    "regression. All three are instances of supervised learning, where the goal is to learn the\n",
    "relationship between training inputs and training targets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- such as optical character\n",
    "recognition, speech recognition, image classification, and language translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sequence generation—Given a picture, predict a caption describing it. Sequence\n",
    "generation can sometimes be reformulated as a series of classification problems\n",
    "(such as repeatedly predicting a word or token in a sequence).\n",
    "- Syntax tree prediction—Given a sentence, predict its decomposition into a syntax\n",
    "tree.\n",
    "- Object detection—Given a picture, draw a bounding box around certain objects\n",
    "inside the picture. This can also be expressed as a classification problem (given\n",
    "many candidate bounding boxes, classify the contents of each one) or as a joint\n",
    "classification and regression problem, where the bounding-box coordinates are\n",
    "predicted via vector regression.\n",
    "- Image segmentation—Given a picture, draw a pixel-level mask on a specific object. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Unsupervised learning\n",
    "-  finding interesting transformations of the\n",
    "input data without the help of any targets, for the purposes of data visualization, data\n",
    "compression, or data denoising, or to better understand the correlations present in\n",
    "the data at hand\n",
    "-  Unsupervised learning is the bread and butter of data analytics, and\n",
    "it’s often a necessary step in better understanding a dataset \n",
    "- Dimensionality reduction and clustering are well-known\n",
    "categories of unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.3 Self-supervised learning\n",
    "- This is a specific instance of supervised learning, but it’s different enough that it\n",
    "deserves its own category.\n",
    "- Self-supervised learning is supervised learning without human-annotated labels.\n",
    "- There are still labels involved (because the learning has to be\n",
    "supervised by something), but they’re generated from the input data, typically using a\n",
    "heuristic algorithm\n",
    "- <b>autoencoders</b> are a well-known instance of self-supervised learning,\n",
    "where the generated targets are the input, unmodified. In the same way, trying to predict\n",
    "the next frame in a video, given past frames, or the next word in a text, given previous\n",
    "words, are instances of self-supervised learning (temporally supervised learning, in this\n",
    "case: supervision comes from future input data). Note that the distinction between\n",
    "supervised, self-supervised, and unsupervised learning can be blurry sometimes\n",
    "\n",
    "- <b>NOTE</b> In this book, we’ll focus specifically on supervised learning, because\n",
    "it’s by far the dominant form of deep learning today, with a wide range of\n",
    "industry applications. We’ll also take a briefer look at self-supervised learning\n",
    "in later chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4 Reinforcement learning\n",
    "-  In reinforcement learning,\n",
    "an agent receives information about its environment and learns to choose actions that\n",
    "will maximize some reward. For instance, a neural network that “looks” at a videogame\n",
    "screen and outputs game actions in order to maximize its score can be trained\n",
    "via reinforcement learning\n",
    "- we expect to see reinforcement\n",
    "learning take over an increasingly large range of real-world applications:\n",
    "self-driving cars, robotics, resource management, education, and so on. It’s an idea\n",
    "whose time has come, or will come soon. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Classification and regression glossary<Br>They have precise, machine-learning-specific definitions, and you should be familiar\n",
    "with them:\n",
    "  - Sample or input—One data point that goes into your model.\n",
    "  - Prediction or output—What comes out of your model.\n",
    "  - Target—The truth. What your model should ideally have predicted, according\n",
    "to an external source of data.\n",
    "  - Prediction error or loss value—A measure of the distance between your\n",
    "model’s prediction and the target.\n",
    "  - Classes—A set of possible labels to choose from in a classification problem.\n",
    "For example, when classifying cat and dog pictures, “dog” and “cat” are the\n",
    "two classes.\n",
    "  - Label—A specific instance of a class annotation in a classification problem.\n",
    "For instance, if picture #1234 is annotated as containing the class “dog,”\n",
    "then “dog” is a label of picture #1234.\n",
    "  - Ground-truth or annotations—All targets for a dataset, typically collected by\n",
    "humans.\n",
    "  - Binary classification—A classification task where each input sample should\n",
    "be categorized into two exclusive categories\n",
    "  - Multiclass classification—A classification task where each input sample\n",
    "should be categorized into more than two categories: for instance, classifying\n",
    "handwritten digits.\n",
    "  - Multilabel classification—A classification task where each input sample can\n",
    "be assigned multiple labels. For instance, a given image may contain both a\n",
    "cat and a dog and should be annotated both with the “cat” label and the\n",
    "“dog” label. The number of labels per image is usually variable.\n",
    "  - Scalar regression—A task where the target is a continuous scalar value. Predicting\n",
    "house prices is a good example: the different target prices form a continuous\n",
    "space.\n",
    "  - Vector regression—A task where the target is a set of continuous values: for\n",
    "example, a continuous vector. If you’re doing regression against multiple values\n",
    "(such as the coordinates of a bounding box in an image), then you’re\n",
    "doing vector regression.\n",
    "  - Mini-batch or batch—A small set of samples (typically between 8 and 128)\n",
    "that are processed simultaneously by the model. The number of samples is\n",
    "often a power of 2, to facilitate memory allocation on GPU. When training, a\n",
    "mini-batch is used to compute a single gradient-descent update applied to\n",
    "the weights of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Evaluating machine-learning models\n",
    "-  training set, a\n",
    "validation set, and a test set.\n",
    "- reason divide our data into two part for removing <b>overfiting</b> problem.\n",
    "- Should our model generalize that perform on never-seen-data\n",
    "- overfiting is big obstacle \n",
    "- we’ll focus on how\n",
    "to measure generalization: how to evaluate machine-learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Training, validation, and test sets\n",
    "-  You train on the training data and evaluate your model simple hold-out validation, Kfold\n",
    "on the validation data. Once your model is ready for prime time, you test it one final\n",
    "time on the test data.\n",
    "- You may ask, why not have two sets: a training set and a test set? You’d train on the\n",
    "training data and evaluate on the test data. Much simpler!\n",
    "-  model always involves tuning its configuration: for\n",
    "example, choosing the number of layers or the size of the layers (called the hyperparameters\n",
    "of the model, to distinguish them from the parameters, which are the network’s\n",
    "weights). You do this tuning by using as a feedback signal the performance of\n",
    "the model on the validation data. In essence, this tuning is a form of learning: a search\n",
    "for a good configuration in some parameter space. As a result, tuning the configuration\n",
    "of  model always involves tuning its configuration: for\n",
    "example, choosing the number of layers or the size of the layers (called the hyperparameters\n",
    "of the model, to distinguish them from the parameters, which are the network’s\n",
    "weights). You do this tuning by using as a feedback signal the performance of\n",
    "the model on the validation data. In essence, this tuning is a form of learning: a search\n",
    "for a good configuration in some parameter space. As a result, tuning the configuration\n",
    "of the model based on its performance on the validation  set can quickly result in\n",
    "overfitting to the validation set, even though your model is never directly trained on it.\n",
    "- If anything about the model has been tuned based on test set performance, then your\n",
    "measure of generalization will be flawed.\n",
    "- validation, and iterated K-fold validation with shuffling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SIMPLE HOLD-OUT VALIDATION\n",
    "Set apart some fraction of your data as your test set. Train on the remaining data, and\n",
    "evaluate on the test set.\n",
    "<img src='images/f4.1.png'>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### <div style='color:#fff; background-color: skyblue;padding:10px 20px;'></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style='color:#fff; background-color: skyblue;padding:10px 20px;'>Listing 4.1 Hold-out validation</div>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "num_validation_samples = 10000\n",
    "\n",
    "np.random.shuffle(data)\n",
    "validation_data = data[:num_validation_samples]\n",
    "data = data[num_validation_samples:]\n",
    "training_data = data[:]\n",
    "model = get_model()\n",
    "model.train(training_data)\n",
    "validation_score = model.evaluate(validation_data)\n",
    "# At this point you can tune your model,\n",
    "# retrain it, evaluate it, tune it again...\n",
    "model = get_model()\n",
    "model.train(np.concatenate([training_data,\n",
    "                            validation_data]))\n",
    "test_score = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-FOLD VALIDATION\n",
    " For each partition\n",
    "i, train a model on the remaining K – 1 partitions, and evaluate it on partition i.\n",
    "Your final score is then the averages of the K scores obtained. \n",
    "<img src='images/f4.2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style='color:#fff; background-color: skyblue;padding:10px 20px;'>Listing 4.2 K-fold cross-validation</div>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "k=4\n",
    "num_validation_samples = len(data) // k\n",
    "np.random.shuffle(data)\n",
    "validation_scores = []\n",
    "for fold in range(k):\n",
    "    validation_data = data[num_validation_samples * fold:\n",
    "                           num_validation_samples * (fold + 1)]\n",
    "    training_data = data[:num_validation_samples * fold] + data[num_validation_samples * (fold + 1):]\n",
    "    model = get_model()\n",
    "    model.train(training_data)\n",
    "    validation_score = model.evaluate(validation_data)\n",
    "    validation_scores.append(validation_score)\n",
    "    \n",
    "validation_score = np.average(validation_scores)\n",
    "model = get_model()\n",
    "model.train(data)\n",
    "test_score = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ITERATED K-FOLD VALIDATION WITH SHUFFLING\n",
    " It consists of applying K-fold validation multiple times, shuffling\n",
    "the data every time before splitting it K ways. The final score is the average of the\n",
    "scores obtained at each run of K-fold validation. Note that you end up training and\n",
    "evaluating P × K models (where P is the number of iterations you use), which can very\n",
    "expensive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 Data preprocessing, feature engineering, and feature learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.1 Data preprocessing for neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing aims at making the raw data at hand more amenable to neural\n",
    "networks. This includes vectorization, normalization, handling missing values, and\n",
    "feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization\n",
    "All inputs and targets in a neural network must be tensors of floating-point data (or, in\n",
    "specific cases, tensors of integers). Whatever data you need to process—sound,\n",
    "images, text—you must first turn into tensors, a step called data vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VALUE NORMALIZATION\n",
    "-  encoding grayscale values. Before you fed this data into your network,\n",
    "you had to cast it to float32 and divide by 255 so you’d end up with floatingpoint\n",
    "values in the 0–1 range. \n",
    "- when predicting house prices, you started\n",
    "from features that took a variety of ranges—some features had small floating-point values,\n",
    "others had fairly large integer values. Before you fed this data into your network,\n",
    "you had to normalize each feature independently so that it had a standard deviation\n",
    "of 1 and a mean of 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - Take small values—Typically, most values should be in the 0–1 range.\n",
    "  - Be homogenous—That is, all features should take values in roughly the same\n",
    "range.Additionally, the following stricter normalization practice is common and can help,\n",
    "although it isn’t always necessary (for example, you didn’t do this in the digit-classification\n",
    "example): \n",
    "  - Normalize each feature independently to have a mean of 0.\n",
    "  - Normalize each feature independently to have a standard deviation of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is easy to do with Numpy arrays:<br>\n",
    "```x -= x.mean(axis=0)\n",
    "x /= x.std(axis=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HANDLING MISSING VALUES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " expecting missing values in the test data, but the network was\n",
    "trained on data without any missing values, the network won’t have learned to ignore\n",
    "missing values! In this situation, you should artificially generate training samples with\n",
    "missing entries: copy some training samples several times, and drop some of the features\n",
    "that you expect are likely to be missing in the test data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2 Feature engineering\n",
    "Feature engineering is the process of using your own knowledge about the data and about\n",
    "the machine-learning algorithm at hand (in this case, a neural network) to make the\n",
    "algorithm work better by applying\n",
    "hardcoded (nonlearned) transformations\n",
    "to the data before it goes\n",
    "into the model\n",
    "<img src='images/f4.3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Fortunately, modern deep learning removes the need for most feature engineering,\n",
    "because neural networks are capable of automatically extracting useful features\n",
    "from raw data. Does this mean you don’t have to worry about feature engineering as\n",
    "long as you’re using deep neural networks? No, for two reasons:\n",
    "- Good features still allow you to solve problems more elegantly while using fewer\n",
    "resources. For instance, it would be ridiculous to solve the problem of reading a\n",
    "clock face using a convolutional neural network.\n",
    "- Good features let you solve a problem with far less data. The ability of deeplearning\n",
    "models to learn features on their own relies on having lots of training\n",
    "data available; if you have only a few samples, then the information value in\n",
    "their features becomes critical. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 Overfitting and underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  The fundamental issue in machine learning is the tension between optimization\n",
    "and generalization. Optimization refers to the process of adjusting a model to get the\n",
    "best performance possible on the training data (the learning in machine learning),\n",
    "whereas generalization refers to how well the trained model performs on data it has\n",
    "never seen before. The goal of the game is to get good generalization, of course, but\n",
    "you don’t control generalization; you can only adjust the model based on its training\n",
    "data\n",
    "- At the beginning of training, optimization and generalization are correlated: the\n",
    "lower the loss on training data, the lower the loss on test data. While this is happening,\n",
    "your model is said to be underfit:\n",
    "- The processing of fighting overfitting this way is called regularization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4.1 Reducing the network’s size\n",
    "- The simplest way to prevent overfitting is to reduce the size of the model: the number\n",
    "of learnable parameters in the model (which is determined by the number of layers\n",
    "and the number of units per layer)\n",
    "- Unfortunately, there is no magical formula to determine the right number of layers\n",
    "or the right size for each layer. You must evaluate an array of different architectures\n",
    "(on your validation set, not on your test set, of course) in order to find the\n",
    "correct model size for your data. The general workflow to find an appropriate model\n",
    "size is to start with relatively few layers and parameters, and increase the size of the layers\n",
    "or add new layers until you see diminishing returns with regard to validation loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style='color:#fff; background-color: skyblue;padding:10px 20px;'>Listing 4.3 Original model</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now let’s try to replace it with this smaller network.\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "print(\"Now let’s try to replace it with this smaller network.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style='color:#fff; background-color: skyblue;padding:10px 20px;'>Listing 4.4 Version of the model with lower capacity</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(4, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(4, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 4.4 shows a comparison of the validation losses of the original network and the\n",
    "smaller network. The dots are the validation loss values of the smaller network, and\n",
    "the crosses are the initial network (remember, a lower validation loss signals a better\n",
    "model)\n",
    "<img src='images/f4.4.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style='color:#fff; background-color: skyblue;padding:10px 20px;'>Listing 4.5 Version of the model with higher capacity</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 4.4.5 shows how the bigger network fares compared to the reference network.\n",
    "The dots are the validation loss values of the bigger network, and the crosses are the\n",
    "initial network.\n",
    "<img src='images/f4.4_5.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bigger network starts overfitting almost immediately, after just one epoch, and it\n",
    "overfits much more severely. Its validation loss is also noisier.\n",
    " Meanwhile, figure 4.6 shows the training losses for the two networks. As you can\n",
    "see, the bigger network gets its training loss near zero very quickly. The more capacity\n",
    "the network has, the more quickly it can model the training data (resulting in a low\n",
    "training loss), but the more susceptible it is to overfitting (resulting in a large difference\n",
    "between the training and validation loss). \n",
    "<img src='images/f4.6.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.2 Adding weight regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  A simple model in this context is a model where the distribution of parameter values\n",
    "has less entropy (or a model with fewer parameters, as you saw in the previous section).\n",
    "Thus a common way to mitigate overfitting is to put constraints on the complexity\n",
    "of a network by forcing its weights to take only small values, which makes the\n",
    "distribution of weight values more regular. This is called weight regularization\n",
    "- done by adding to the loss function of the network a cost associated with having large\n",
    "weights. This cost comes in two flavors:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " L1 regularization—The cost added is proportional to the absolute value of the\n",
    "weight coefficients (the L1 norm of the weights).\n",
    " L2 regularization—The cost added is proportional to the square of the value of the\n",
    "weight coefficients (the L2 norm of the weights). L2 regularization is also called\n",
    "weight decay in the context of neural networks. Don’t let the different name confuse\n",
    "you: weight decay is mathematically the same as L2 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Keras, weight regularization is added by passing weight regularizer instances to layers\n",
    "as keyword arguments. Let’s add L2 weight regularization to the movie-review classification\n",
    "network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style='color:#fff; background-color: skyblue;padding:10px 20px;'>Listing 4.6 Adding L2 weight regularization to the model</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
    "                       activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
    "                       activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l2(0.001) means every coefficient in the weight matrix of the layer will add 0.001 *\n",
    "weight_coefficient_value to the total loss of the network. Note that because this\n",
    "penalty is only added at training time, the loss for this network will be much higher at\n",
    "training than at test time<br>\n",
    " Figure 4.7 shows the impact of the L2 regularization penalty. As you can see, the\n",
    "model with L2 regularization (dots) has become much more resistant to overfitting\n",
    "than the reference model (crosses), even though both models have the same number\n",
    "of parameters.\n",
    "<img src='images/f4.7.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative to L2 regularization, you can use one of the following Keras weight\n",
    "regularizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style='color:#fff; background-color: skyblue;padding:10px 20px;'>Listing 4.7 Different weight regularizers available in Keras</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.regularizers.L1L2 at 0x2052a5d89b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "regularizers.l1(0.001)\n",
    "regularizers.l1_l2(l1=0.001, l2=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4.3 Adding dropout\n",
    "-  Dropout, applied to a layer, consists of randomly dropping out\n",
    "(setting to zero) a number of output features of the layer during training. Let’s say a\n",
    "given layer would normally return a vector [0.2, 0.5, 1.3, 0.8, 1.1] for a given input\n",
    "sample during training. After applying dropout, this vector will have a few zero entries\n",
    "distributed at random: for example, [0, 0.5, 1.3, 0, 1.1]. The dropout rate is the fraction\n",
    "of the features that are zeroed out; it’s usually set between 0.2 and 0.5. At test time, no\n",
    "units are dropped out; instead, the layer’s output values are scaled down by a factor\n",
    "equal to the dropout rate, to balance for the fact that more units are active than at\n",
    "training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a Numpy matrix containing the output of a layer, layer_output, of\n",
    "shape (batch_size, features). At training time, we zero out at random a fraction of\n",
    "the values in the matrix:<br>\n",
    "```\n",
    "layer_output *= np.random.randint(0, high=2, size=layer_output.shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At test time, we scale down the output by the dropout rate. Here, we scale by 0.5\n",
    "(because we previously dropped half the units):\n",
    "```layer_output *= 0.5```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this process can be implemented by doing both operations at training time\n",
    "and leaving the output unchanged at test time, which is often the way it’s implemented\n",
    "in practice (see figure 4.8):\n",
    "```\n",
    "layer_output *= np.random.randint(0, high=2, size=layer_output.shape)\n",
    "layer_output /= 0.5\n",
    "```\n",
    "<img src='images/f4.8.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.add(layers.Dropout(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style='color:#fff; background-color: skyblue;padding:10px 20px;'>Listing 4.8 Adding dropout to the IMDB networkm</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 4.9 shows a plot of the results. Again, this is a clear improvement over the reference\n",
    "network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/f4.9.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To recap, these are the most common ways to prevent overfitting in neural networks:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " Get more training data.\n",
    " Reduce the capacity of the network.\n",
    " Add weight regularization.\n",
    " Add dropout. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.5 The universal workflow of machine learning\n",
    "____________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " universal blueprint that you can use to attack and solve\n",
    "any machine-learning problem. The blueprint ties together the concepts you’ve\n",
    "learned about in this chapter: problem definition, evaluation, feature engineering,\n",
    "and fighting overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5.1 Defining the problem and assembling a dataset\n",
    "- What will your input data be? What are you trying to predict? You can only learn\n",
    "to predict something if you have available training data: for example, you can\n",
    "only learn to classify the sentiment of movie reviews if you have both movie\n",
    "reviews and sentiment annotations available. As such, data availability is usually\n",
    "the limiting factor at this stage (unless you have the means to pay people to collect\n",
    "data for you).\n",
    "- What type of problem are you facing? Is it binary classification? Multiclass classification?\n",
    "Scalar regression? Vector regression? Multiclass, multilabel classification?\n",
    "Something else, like clustering, generation, or reinforcement learning?\n",
    "Identifying the problem type will guide your choice of model architecture, loss\n",
    "function, and so on.<br><br>\n",
    "You can’t move to the next stage until you know what your inputs and outputs are, and\n",
    "what data you’ll use. Be aware of the hypotheses you make at this stage:\n",
    "1. You hypothesize that your outputs can be predicted given your inputs.\n",
    "2. You hypothesize that your available data is sufficiently informative to learn the\n",
    "relationship between inputs and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5.2 Choosing a measure of success\n",
    "<img src='images/recall.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5.3 Deciding on an evaluation protocol"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " Maintaining a hold-out validation set—The way to go when you have plenty of\n",
    "data\n",
    " Doing K-fold cross-validation—The right choice when you have too few samples\n",
    "for hold-out validation to be reliable\n",
    " Doing iterated K-fold validation—For performing highly accurate model evaluation\n",
    "when little data is available\n",
    "Just pick one of these. In most cases, the first will work well enough. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5.4 Preparing your data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " As you saw previously, your data should be formatted as tensors.\n",
    " The values taken by these tensors should usually be scaled to small values: for\n",
    "example, in the [-1, 1] range or [0, 1] range.\n",
    " If different features take values in different ranges (heterogeneous data), then\n",
    "the data should be normalized.\n",
    " You may want to do some feature engineering, especially for small-data problems.\n",
    "Once your tensors of input data and target data are ready, you can begin to train models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5.5 Developing a model that does better than a baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Assuming that things go well, you need to make three key choices to build your\n",
    "first working model:\n",
    "- Last-layer activation—This establishes useful constraints on the network’s output.\n",
    "For instance, the IMDB classification example used sigmoid in the last\n",
    "layer; the regression example didn’t use any last-layer activation; and so on.\n",
    "- Loss function—This should match the type of problem you’re trying to solve. For\n",
    "instance, the IMDB example used binary_crossentropy, the regression example\n",
    "used mse, and so on.\n",
    "- Optimization configuration—What optimizer will you use? What will its learning\n",
    "rate be? In most cases, it’s safe to go with rmsprop and its default learning rate.\n",
    "-  In general, you\n",
    "can hope that the lower the crossentropy gets, the higher the ROC AUC will be.\n",
    " Table 4.1 can help you choose a last-layer activation and a loss function for a few\n",
    "common problem types.\n",
    "<img src='images/4.1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.5.6 Scaling up: developing a model that overfits\n",
    "     Remember that the universal tension in machine learning is between\n",
    "optimization and generalization; the ideal model is one that stands right at the border\n",
    "between underfitting and overfitting; between undercapacity and overcapacity. To figure\n",
    "out where this border lies, first you must cross it.\n",
    " To figure out how big a model you’ll need, you must develop a model that overfits.\n",
    "This is fairly easy:\n",
    "1. Add layers.\n",
    "2. Make the layers bigger.\n",
    "3. Train for more epochs.\n",
    "\n",
    "Always monitor the training loss and validation loss, as well as the training and validation\n",
    "values for any metrics you care about. When you see that the model’s performance\n",
    "on the validation data begins to degrade, you’ve achieved overfitting.\n",
    " The next stage is to start regularizing and tuning the model, to get as close as possible\n",
    "to the ideal model that neither underfits nor overfits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5.7 Regularizing your model and tuning your hyperparameters\n",
    "This step will take the most time: you’ll repeatedly modify your model, train it, evaluate\n",
    "on your validation data (not the test data, at this point), modify it again, and\n",
    "repeat, until the model is as good as it can get. These are some things you should try"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " Add dropout.\n",
    " Try different architectures: add or remove layers.\n",
    " Add L1 and/or L2 regularization.\n",
    " Try different hyperparameters (such as the number of units per layer or the\n",
    "learning rate of the optimizer) to find the optimal configuration.\n",
    " Optionally, iterate on feature engineering: add new features, or remove features\n",
    "that don’t seem to be informative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you’ve developed a satisfactory model configuration, you can train your final\n",
    "production model on all the available data (training and validation) and evaluate it\n",
    "one last time on the test set. If it turns out that performance on the test set is significantly\n",
    "worse than the performance measured on the validation data, this may mean\n",
    "either that your validation procedure wasn’t reliable after all, or that you began overfitting\n",
    "to the validation data while tuning the parameters of the model. In this case,\n",
    "you may want to switch to a more reliable evaluation protocol (such as iterated K-fold\n",
    "validation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter summary\n",
    "- Define the problem at hand and the data on which you’ll train. Collect\n",
    "this data, or annotate it with labels if need be.\n",
    "- Choose how you’ll measure success on your problem. Which metrics will\n",
    "you monitor on your validation data?\n",
    "- Determine your evaluation protocol: hold-out validation? K-fold validation?\n",
    "Which portion of the data should you use for validation?\n",
    "- Develop a first model that does better than a basic baseline: a model with\n",
    "statistical power.\n",
    "- Develop a model that overfits.\n",
    "- Regularize your model and tune its hyperparameters, based on performance\n",
    "on the validation data. A lot of machine-learning research tends to\n",
    "focus only on this step—but keep the big picture in mind."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
