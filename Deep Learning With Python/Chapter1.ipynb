{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "h4{\n",
    "text-align:center;\n",
    "}\n",
    "</style>\n",
    "<!--\n",
    "Authour Name: Muhammad Qasim\n",
    "Version: 1.0\n",
    "Dated: 3-March-2018\n",
    "-->\n",
    "# What is deep learning?\n",
    "## This chapter covers\n",
    "- High-level definitions of fundamental concepts\n",
    "- Timeline of the development of machine learning\n",
    "- Key factors behind deep learning’s rising\n",
    "- popularity and future potential\n",
    "<img src='images/ai_ml_dl.png'>\n",
    "#### Figure 1.1 Artificial intelligence, <br> machine learning, and deep learning\n",
    "<br><br>\n",
    "### 1.1.1 Artificial intelligence\n",
    "- Artificial intelligence was born in the 1950s,\n",
    "- the effort to automate intellectual tasks normally performed\n",
    "by humans\n",
    "- `symbolic`<br>\n",
    "   ```\n",
    "humans input rules (a program) and data to\n",
    "be processed according to these rules, and out come answers (see figure 1.2)\n",
    "   ```\n",
    "complex, fuzzy problems, such as image classification, speech recognition, and language\n",
    "translation. A new approach arose to take symbolic AI’s place: `machine learning`\n",
    "## 1.1.2 Machine learning\n",
    "- Also call <b>Classical programming</b> <br>\n",
    "Machine learning arises from this question: could a computer go beyond \n",
    "* “what weknow how to order it to perform” and learn on its own how to perform a specified task?\n",
    "* Could a computer surprise us? \n",
    "* Rather than programmers crafting data-processing rules by hand, could a computer automatically learn these rules by looking at data?<br>\n",
    "<b>machine learning, humans input data as well as the answers expected from the data,\n",
    "and out come the rules. These rules can then be applied to new data to produce original\n",
    "answers.</b>\n",
    "<img src='images/ai_ml.png'>\n",
    "#### Figure 1.2 Machine learning: a new programming paradigm\n",
    "- Machine learning is tightly related to mathematical statistics, \n",
    "## 1.1.3 Learning representations from data\n",
    "\n",
    "To define `deep learning `and understand the difference between deep learning\n",
    "and other machine-learning approaches, first we need some idea of what machinelearning\n",
    "algorithms `do`. I just stated that machine learning discovers rules to execute\n",
    "a data-processing task, given examples of what’s expected. So, to do machine learning,\n",
    "we need three things:\n",
    "\n",
    "- `Input data points`—For instance, if the task is speech recognition, these data\n",
    "  points could be sound files of people speaking. If the task is image tagging,\n",
    "  they could be pictures.\n",
    "- `Examples of the expected output`—In a speech-recognition task, these could be\n",
    "  human-generated transcripts of sound files. In an image task, expected outputs\n",
    "  could be tags such as “dog,” “cat,” and so on.\n",
    "- A `way to measure whether the algorithm is doing a good job`—This is necessary in\n",
    "  order to determine the distance between the algorithm’s current output and\n",
    "  its expected output. The measurement is used as a feedback signal to adjust\n",
    "  the way the algorithm works. This adjustment step is what we call learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- meaningfully\n",
    "  ransform data:\n",
    "- representations<BR>\n",
    "  what’s a representation? At its core, it’s a different way to look at data—to represent\n",
    "  or encode data Machine-learning models are all about finding appropriate representations\n",
    "  for their input data—transformations of the data that make it more amenable\n",
    "  to the task at hand, such as a classification task\n",
    "  <img src=\"images/f1.4.png\">\n",
    "  the\n",
    "black/white classification problem can be expressed as a simple rule: “Black points\n",
    "are such that x > 0,” or “White points are such that x < 0.” This new representation\n",
    "basically solves the classification problemfads<br>\n",
    "<b>finding these transformations; they’re merely searching through a predefined set of\n",
    "operations, called a `hypothesis space`.</b><br>\n",
    "```\n",
    "So that’s what machine learning is, technically: searching for useful representations\n",
    "of some input data, within a predefined space of possibilities, using guidance\n",
    "from a feedback signal. This simple idea allows for solving a remarkably broad range\n",
    "of intellectual tasks, from speech recognition to autonomous car driving.\n",
    "   Now that you understand what we mean by learning, let’s take a look at what makes\n",
    "deep learning special.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.4 The “deep” in deep learning\n",
    "- How many layers contribute to a model of the data is\n",
    "  called the depth of the model. \n",
    "- Other appropriate names for the field could have been\n",
    "  layered representations learning and hierarchical representations learning\n",
    "- Meanwhile,\n",
    "  other approaches to machine learning tend to focus on learning only one or two layers\n",
    "  of representations of the data; hence, they’re sometimes called shallow learning\n",
    "- In deep learning, these layered representations are (almost always) learned via\n",
    "  models called neural networks, structured in literal layers stacked on top of each other.\n",
    "  \n",
    "- mystique and mystery, and\n",
    "  you may as well forget anything you may have read about hypothetical links between\n",
    "  deep learning and biology. For our purposes, deep learning is a ,<b>mathematical framework</b>\n",
    "  for learning representations from data.\n",
    " <img src=\"images/f4.5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in figure 1.6, the network transforms the digit image into representations\n",
    "that are increasingly different from the original image and increasingly informative\n",
    "about the final result. You can think of a deep network as a multistage\n",
    "information-distillation operation, where information goes through successive filters\n",
    "and comes out increasingly purified (that is, useful with regard to some task).\n",
    "<img src=\"images/f1.6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ###  multistage way to learn data representations\n",
    "____\n",
    "\n",
    "\n",
    "## 1.1.5 Understanding how deep learning works, in three figures\n",
    "- machine learning is about mapping inputs (such as\n",
    "  images) to targets (such as the label “cat”).\n",
    "- deep neural networks do this input-to-target\n",
    "- mapping via a deep sequence of simple data transformations (layers) \n",
    "- input data is stored in the layer’s <b>weights</b>. <i>In technical terms, we’d say that the\n",
    "transformation implemented by a layer is parameterized by its weights</i> (see figure 1.7).\n",
    "(Weights are also sometimes called the parameters of a layer.)\n",
    "-  <b>learning</b>: \n",
    "means finding a set of values for the weights of all layers in a network\n",
    "<img src='images/f1.7.png'>\n",
    "-  to measure how far this output is from what you expected\n",
    "- This is the job of the loss function of the network, also called the objective function.\n",
    "- how well the network has done on this specific example (see figure 1.8).\n",
    "<img src='images/f1.8.png'>\n",
    "#####  optimizer:\n",
    "- The fundamental trick in deep learning is to use this score as a feedback signal to\n",
    "adjust the value of the weights a little, in a direction that will lower the loss score for\n",
    "the current example (see figure 1.9), which\n",
    "implements what’s called the Backpropagation algorithm\n",
    "<img src='images/f1.9.png'>\n",
    "- the weights of the network are assigned random values\n",
    "- the correct\n",
    "direction, and the loss score decreases  This is the training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.6 What deep learning has achieved so far\n",
    "- seeing and hearing\n",
    "- Near-human-level image classification\n",
    "- Near-human-level speech recognition\n",
    "- Near-human-level handwriting transcription\n",
    "- Improved machine translation\n",
    "- Improved text-to-speech conversion\n",
    "- Digital assistants such as Google Now and Amazon Alexa\n",
    "- Near-human-level autonomous driving\n",
    "- Improved ad targeting, as used by Google, Baidu, and Bing\n",
    "- Improved search results on the web\n",
    "- Ability to answer natural-language questions\n",
    "- Superhuman Go playing\n",
    "\n",
    "## 1.1.7 Don’t believe the short-term hype\n",
    "\n",
    "____\n",
    "## 1.1.8 The promise of AI\n",
    "-  it will answer your questions,\n",
    "help educate your kids, and watch over your health. It will deliver your groceries to your\n",
    "door and drive you from point A to point B\n",
    "\n",
    "# 1.2 Before deep learning: \n",
    "## a brief history of machine learning\n",
    "- the machine-learning algorithms used in the industry\n",
    "today aren’t deep-learning algorithms\n",
    "- you may find yourself in a situation where all\n",
    "you have is the deep-learning hammer, and every machine-learning problem starts to\n",
    "look like a nail\n",
    "-  deep learning in the broader\n",
    "context of machine learning and better understand where deep learning comes from\n",
    "and why it matters\n",
    "\n",
    "## 1.2.1 Probabilistic modeling\n",
    "- <b>Probabilistic modeling</b> is the application of the principles of statistics to data analysis\n",
    "- was one of the earliest forms of machine learning\n",
    "- One of the best-known algorithms in this category is the Naive Bayes algorithm\n",
    "-  Naive Bayes is a type of machine-learning classifier based on applying Bayes’ theorem\n",
    "while assuming that the features in the input data are all independent (a strong,\n",
    "or “naive” assumption, which is where the name comes from).\n",
    "-  logistic regression (logreg for short),name—logreg is a classification algorithm rather than a regression\n",
    "algorithm. \n",
    "- data scientist will try on a dataset to get a feel for the classification task at hand\n",
    "\n",
    "## 1.2.2 Early neural networks\n",
    "- f neural networks were investigated in toy forms as early\n",
    "as the 1950s,\n",
    "- the mid-1980s,<br>when multiple people independently rediscovered the Backpropagation algorithm—\n",
    "a way to train chains of parametric operations using gradient-descent optimization\n",
    "- The first successful practical application of neural nets came in 1989 from Bell\n",
    "Labs, \n",
    "- when Yann LeCun combined the earlier ideas of convolutional neural networks\n",
    "and backpropagation, and applied them to the problem of classifying handwritten\n",
    "digits. <br>\n",
    "LeNet,\n",
    "-  the United States Postal Service\n",
    "in the 1990s to automate the reading of ZIP codes on mail envelopes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.3 Kernel methods\n",
    "- quickly sent neural nets back to oblivion: kernel methods.\n",
    "- Kernel methods are a group of\n",
    "classification algorithms\n",
    "- support vector machine (SVM).\n",
    "- SVM was Developed in 1990 at bell labs\n",
    "- SVMs aim at solving classification problems by finding good\n",
    "decision boundaries (see figure 1.10) \n",
    "- A decision boundary can\n",
    "be thought of as a line or surface separating your training data\n",
    "into two spaces corresponding to two categories\n",
    "<img src='images/f1.10.png'>\n",
    "- SVMs proceed to find these boundaries in two steps:\n",
    "    - The data is mapped to a new high-dimensional representation where the\n",
    "decision boundary can be expressed as a hyperplane (if the data was twodimensional,\n",
    "as in figure 1.10, a hyperplane would be a straight line).\n",
    "    - A good decision boundary (a separation hyperplane) is computed by trying to\n",
    "maximize the distance between the hyperplane and the closest data points from\n",
    "each class, a step called <b>maximizing the margin</b>. This allows the boundary to generalize\n",
    "well to new samples outside of the training dataset.\n",
    "- the distance\n",
    "between pairs of points in that space, which can be done efficiently using a kernel\n",
    "function.\n",
    "\n",
    "- A kernel function is a computationally tractable operation that maps any\n",
    "two points in your initial space to the distance between these points in your target\n",
    "representation space, completely bypassing the explicit computation of the new representation.\n",
    "-  But SVMs proved hard to scale to large datasets and didn’t provide good results for\n",
    "perceptual problems such as image classification\n",
    "- Because an SVM is a shallow\n",
    "method, applying an SVM to perceptual problems requires first extracting useful representations\n",
    "manually (a step called feature engineering), which is difficult and brittle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.4 Decision trees, random forests, and gradient boosting machines\n",
    "- Decision trees are flowchart-like structures \n",
    "- classify input data points or predict\n",
    "output values given inputs (see figure 1.11).\n",
    "- Decisions trees learned from data\n",
    "<img src='images/f1.11.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>Random Forest algorithm:</b><br>\n",
    "- random forests quickly\n",
    "became a favorite on the platform—until 2014,\n",
    "- </b>gradient boosting machines</b><br> took\n",
    "over. A gradient boosting machine, much like a random forest, is a machine-learning\n",
    "technique based on ensembling weak prediction models, generally decision trees.\n",
    "- the gradient boosting technique results in models\n",
    "that strictly outperform random forests most of the time\n",
    "## 1.2.5 Back to neural networks\n",
    "-  In 2011, Dan Ciresan from IDSIA began to win academic image-classification competitions\n",
    "with GPU-trained deep neural networks—the first practical success of modern\n",
    "deep learning.\n",
    "- top-five accuracy of the winning model, based on classical\n",
    "approaches to computer vision, was only 74.3%. Then, in 2012,\n",
    "- Geoffrey Hinton was able to achieve a top-five accuracy of\n",
    "83.6%—a significant breakthrough\n",
    "-  By 2015, the winner reached an accuracy\n",
    "of 96.4%, and the classification task on ImageNet\n",
    "-  Since 2012, deep convolutional neural networks (convnets) have become the go-to\n",
    "algorithm for all computer vision tasks\n",
    "- At major computer vision conferences in 2015 and 2016, it was nearly impossible\n",
    "to find presentations that didn’t involve convnets in some form. At the same time,\n",
    "deep learning has also found applications in many other types of problems, such as\n",
    "natural-language processing. It has completely replaced SVMs and decision trees in a\n",
    "wide range of applications\n",
    "- Nuclear Research, CERN, used decision tree–based methods for analysis of particle\n",
    "data from the ATLAS detector at the Large Hadron Collider (LHC);\n",
    "- but CERN eventually\n",
    "switched to Keras-based deep neural networks due to their higher performance\n",
    "and ease of training on large datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.6 What makes deep learning different\n",
    "- The primary reason deep learning took off so quickly is that it offered better performance\n",
    "on many problems.\n",
    "- problem-solving much easier\n",
    "-  it completely automates what used to be the\n",
    "most crucial step in a machine-learning workflow: feature engineering\n",
    "- Previous machine-learning techniques—shallow learning—only involved transforming\n",
    "the input data into one or two successive representation spaces, usually via\n",
    "simple transformations such as high-dimensional non-linear projections (SVMs) or\n",
    "decision trees\n",
    "- the refined representations required by complex problems generally\n",
    "can’t be attained by such techniques. \n",
    "-  humans had to go to great lengths\n",
    "to make the initial input data more amenable to processing by these methods:\n",
    "-  they\n",
    "had to manually engineer good layers of representations for their data. This is called\n",
    "feature engineering\n",
    "-  <b>Deep learning</b><br>, on the other hand, completely automates this step:\n",
    "with deep learning, you learn all features in one pass rather than having to engineer\n",
    "them yourself.<br><br>\n",
    "This has greatly simplified machine-learning workflows, often replacing\n",
    "sophisticated multistage pipelines with a single\n",
    "- In practice, there are fast-diminishing returns to successive applications\n",
    "of shallow-learning methods, because the optimal first representation layer in a threelayer\n",
    "model isn’t the optimal first layer in a one-layer or two-layer model\n",
    "- deep learning is that it allows a model to learn all layers of representation\n",
    "jointly, at the same time, rather than in succession (greedily, as it’s called).\n",
    "- Everything is supervised by a single feedback signal: every change in the\n",
    "    model serves the end goal \n",
    "-  These are the two essential characteristics of how deep learning learns from data:\n",
    "- the incremental, layer-by-layer way in which increasingly complex representations are developed,\n",
    "and the fact that these intermediate incremental representations are learned jointly, each layer\n",
    "being updated to follow both the representational needs of the layer above and the\n",
    "needs of the layer below. \n",
    "\n",
    "## 1.2.7 The modern machine-learning landscape\n",
    "- gradient boosting is used for problems\n",
    "where structured data is available\n",
    "- whereas deep learning is used for perceptual problems\n",
    "such as image classification. \n",
    "-  gradient boosting machines, for shallowlearning\n",
    "problems; and deep learning, for perceptual problems. In technical terms,\n",
    "this means you’ll need to be familiar with XGBoost and Keras—the two libraries that\n",
    "currently dominate Kaggle competitions. With this book in hand, you’re already one\n",
    "big step closer.\n",
    "-------------------\n",
    "\n",
    "## 1.3 Why deep learning? Why now?\n",
    "-----------------\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Long ShortTerm\n",
    "Memory (LSTM) algorithm, which is fundamental to deep learning for\n",
    "timeseries, was developed in 1997 and has barely changed since. So why did deep\n",
    "learning only take off after 2012? What changed in these two decades?\n",
    " In general, three technical forces are driving advances in machine learning:\n",
    " - Hardware\n",
    " - Datasets and benchmarks\n",
    " - Algorithmic advances\n",
    " \n",
    "Machine learning isn’t\n",
    "mathematics or physics, where major advances can be done with a pen and a piece of\n",
    "paper. It’s an engineering science.\n",
    "- The real bottlenecks throughout the 1990s and 2000s were data and hardware\n",
    "## 1.3.1 Hardware\n",
    "- deep-learning models used in computer vision or speech recognition\n",
    "require orders of magnitude more computational power than what your laptop can\n",
    "deliver. \n",
    "- A small number of GPUs started replacing\n",
    "massive clusters of CPUs in various highly parallelizable applications\n",
    "-   NVIDIA TITAN X\n",
    "- Google revealed its tensor processing unit (TPU) project: a\n",
    "new chip design developed from the ground up to run deep neural networks, which is\n",
    "reportedly 10 times faster and far more energy efficient than top-of-the-line GPUs. \n",
    "\n",
    "## 1.3.2 Data\n",
    "-  data is its coal\n",
    "- without which nothing would be possible\n",
    "- User-generated image tags on\n",
    "Flickr, for instance, have been a treasure trove of data for computer vision.\n",
    "- the\n",
    "ImageNet dataset, consisting of 1.4 million images that have been hand annotated\n",
    "with 1,000 image categories (1 category per image).\n",
    "\n",
    "## 1.3.3 Algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hardware and data, until the late 2000s, we were missing a reliable way to\n",
    "train very deep neural networks only one or two layers of representations\n",
    "- The key issue was that\n",
    "of gradient propagation through deep stacks of layers\n",
    "- The key issue was that\n",
    "of gradient propagation through deep stacks of layers\n",
    "- This changed around 2009–2010 with the advent of several simple but important\n",
    "algorithmic improvements that allowed for better gradient propagation:\n",
    "    - Better <b>activation functions</b> for neural layers\n",
    "    - Better <b>weight-initialization schemes</b>, starting with layer-wise pretraining, which was\n",
    "quickly abandoned\n",
    "    - Better <b>optimization schemes</b>, such as RMSProp and Adam\n",
    "- Only when these improvements began to allow for training models with 10 or more\n",
    "layers did deep learning start to shine.\n",
    "- 2014, 2015, and 2016, even more advanced ways to help gradient propagation\n",
    "were discovered\n",
    "    -  batch normalization\n",
    "    -  residual connections, and depthwise\n",
    "separable convolutions\n",
    "## 1.3.4 A new wave of investment\n",
    "- In 2013, Google acquired the deep-learning startup DeepMind for a\n",
    "reported $500 million—the largest acquisition of an AI company in history. In 2014,\n",
    "Baidu started a deep-learning research center in Silicon Valley, investing $300 million\n",
    "in the project\n",
    "- The deep-learning hardware startup Nervana Systems was acquired by\n",
    "Intel in 2016 for over $400 million\n",
    "\n",
    "## 1.3.5 The democratization of deep learning\n",
    "- In the early days, doing deep learning\n",
    "required significant C++ and CUDA expertise, which few people possessed. Nowadays,\n",
    "basic Python scripting skills suffice to do advanced deep-learning research\n",
    "- two symbolic\n",
    "tensor-manipulation frameworks for Python that support autodifferentiation\n",
    "- user-friendly libraries\n",
    "such as Keras, which makes deep learning as easy as manipulating LEGO bricks\n",
    "- deep-learning solution for large\n",
    "numbers of new startups, graduate students, and researchers pivoting into the field. \n",
    "## 1.3.6 Will it last?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deep learning has several properties that justify its status as an AI revolution\n",
    "- These important properties can be broadly sorted into three categories:\n",
    "    - Simplicity—Deep learning removes the need for feature engineering, replacing\n",
    "complex, brittle, engineering-heavy pipelines with simple, end-to-end trainable\n",
    "models that are typically built using only five or six different tensor operations.\n",
    "    - Scalability—Deep learning is highly amenable to parallelization on GPUs or\n",
    "TPUs, so it can take full advantage of Moore’s law. In addition, deep-learning\n",
    "models are trained by iterating over small batches of data, allowing them to be\n",
    "trained on datasets of arbitrary size. (The only bottleneck is the amount of\n",
    "parallel computational power available, which, thanks to Moore’s law, is a fastmoving\n",
    "barrier.)\n",
    "    - Versatility and reusability—Unlike many prior machine-learning approaches,\n",
    "deep-learning models can be trained on additional data without restarting from\n",
    "scratch, making them viable for continuous online learning—an important\n",
    "property for very large production models. Furthermore, trained deep-learning\n",
    "models are repurposable and thus reusable: for instance, it’s possible to take a\n",
    "deep-learning model trained for image classification and drop it into a videoprocessing\n",
    "pipeline. This allows us to reinvest previous work into increasingly\n",
    "    - complex and powerful models. This also makes deep learning applicable to\n",
    "fairly small datasets\n",
    "-  Deep learning in 2017 seems to be in the\n",
    "first half of that sigmoid, with much more progress to come in the next few years. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thanks To My Dearest Teachers \n",
    "- Sir Zia Khan\n",
    "- Sir Inamul haq\n",
    "- Sir Saluh Din \n",
    "<img src='images/2.jpg'>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
